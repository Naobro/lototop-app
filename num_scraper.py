import requests
from bs4 import BeautifulSoup
import pandas as pd
import os

def scrape_numbers():
    url = "https://www.hpfree.com/numbers/"
    response = requests.get(url)
    soup = BeautifulSoup(response.content, "html.parser")

    try:
        print("ğŸš€ ãƒŠãƒ³ãƒãƒ¼ã‚º4ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­...")

        # å›å·ã¨æŠ½é¸æ—¥ã®å–å¾—
        title_section = soup.find("h1", class_="topTitle")
        title_text = title_section.text.strip()
        # å›å·ã¨æŠ½é¸æ—¥ã‚’åˆ†å‰²
        round_info = title_text.split("ç¬¬")[1].split("å›")
        round_number = round_info[0]
        draw_date = round_info[1].strip()

        # ãƒŠãƒ³ãƒãƒ¼ã‚º4 å½“é¸ç•ªå·ã®å–å¾— (altå±æ€§ã‹ã‚‰)
        numbers_section = soup.find("section", id="result-Area")
        images = numbers_section.find_all("img")
        
        # ãƒŠãƒ³ãƒãƒ¼ã‚º4ã¯4ã¤ã®ç”»åƒã‹ã‚‰æŠ½å‡º
        numbers = [img["alt"] for img in images[:4]]  # 4ã¤ã®ç”»åƒã‚’å–å¾—
        print(f"ãƒŠãƒ³ãƒãƒ¼ã‚º4 å½“é¸ç•ªå·: {'-'.join(numbers)}")

        # å½“é¸å£æ•°ã¨é…å½“é‡‘ã®å–å¾— (ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰)
        prize_section = soup.find("section", id="table-Area")
        table = prize_section.find("table", class_="table1")
        rows = table.find_all("tr")

        prize_details = []  # ã™ã¹ã¦ã®è¡Œã‚’ã“ã“ã«æ ¼ç´ã™ã‚‹

        # è³é‡‘ã‚¿ã‚¤ãƒ—ã‚’æ‰‹å‹•ã§æŒ‡å®š
        prize_types = ["ã‚¹ãƒˆãƒ¬ãƒ¼ãƒˆ", "ãƒœãƒƒã‚¯ã‚¹", "ã‚»ãƒƒãƒˆãƒ»ã‚¹ãƒˆãƒ¬ãƒ¼ãƒˆ", "ã‚»ãƒƒãƒˆãƒ»ãƒœãƒƒã‚¯ã‚¹", "ãƒŸãƒ‹"]

        # ãƒ‡ãƒãƒƒã‚°: rowsã®å†…å®¹ã‚’ç¢ºèª
        print(f"rows: {len(rows)}")  # rowsã®æ•°ã‚’ç¢ºèª
        for row in rows:
            print(row.text.strip())  # å„è¡Œã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¡¨ç¤º

        for idx, row in enumerate(rows[1:]):  # æœ€åˆã®è¡Œã¯ãƒ˜ãƒƒãƒ€ãƒ¼ãªã®ã§ã‚¹ã‚­ãƒƒãƒ—
            cols = row.find_all("td")
            # ãƒ‡ãƒãƒƒã‚°: colsã®å†…å®¹ã‚’ç¢ºèª
            print(f"cols: {[col.text.strip() for col in cols]}")  # å„åˆ—ã®å†…å®¹ã‚’è¡¨ç¤º
            if len(cols) == 2:  # æ­£ã—ã„è¡Œã¯2ã¤ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŒã£ã¦ã„ã‚‹å ´åˆ
                # "ãƒŸãƒ‹"ã®è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
                if "ãƒŸãƒ‹" in prize_types[idx]:
                    continue

                prize_details.append({
                    "ã‚¿ã‚¤ãƒ—": prize_types[idx] if idx < len(prize_types) else "ä¸æ˜",  # è³é‡‘ã‚¿ã‚¤ãƒ—
                    "å½“é¸å£æ•°": cols[0].text.strip(),  # å£æ•°
                    "å½“é¸é‡‘é¡": cols[1].text.strip(),  # å½“é¸é‡‘é¡
                    "å½“é¸ç•ªå·": "-".join(numbers),  # å½“é¸ç•ªå·ã‚’ä¸€ç·’ã«ä¿å­˜
                    "å›å·": round_number,  # å›å·
                    "æŠ½é¸æ—¥": draw_date   # æŠ½é¸æ—¥
                })

        # ãƒ‡ãƒãƒƒã‚°: prize_detailsãŒç©ºã§ãªã„ã‹ç¢ºèª
        if not prize_details:
            print("âŒ prize_detailsã¯ç©ºã§ã™ã€‚")
        else:
            print("ğŸ¯ ãƒ‡ãƒ¼ã‚¿ãŒæ­£å¸¸ã«å–å¾—ã•ã‚Œã¾ã—ãŸã€‚")

        # ãƒ‡ãƒãƒƒã‚°: å½“é¸ç•ªå·ã¨è³é‡‘ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º
        print("ğŸ¯ ãƒŠãƒ³ãƒãƒ¼ã‚º4ã®æœ€æ–°å½“é¸ç•ªå·ã‚’ 'data/' ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ã—ã¾ã—ãŸã€‚âœ…")
        print(f"ğŸ“Š æœ€æ–°ã®å½“é¸ç•ªå·: {'-'.join(numbers)}")
        for item in prize_details:
            print(item)

        # DataFrameã«å¤‰æ›ã—ã¦CSVã«ä¿å­˜
        data_dir = "/Users/naokinishiyama/loto-prediction-app/data"
        os.makedirs(data_dir, exist_ok=True)

        if prize_details:  # prize_detailsãŒç©ºã§ãªã„å ´åˆã«ã®ã¿CSVã‚’æ›¸ãè¾¼ã‚€
            df_prizes = pd.DataFrame(prize_details)
            numbers_csv_path = os.path.join(data_dir, "numbers_4_latest.csv")
            df_prizes.to_csv(numbers_csv_path, index=False, encoding="utf-8-sig", mode="w")
            print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãŒ {numbers_csv_path} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚")
        else:
            print("âŒ CSVã¸ã®æ›¸ãè¾¼ã¿ã¯è¡Œã„ã¾ã›ã‚“ã€‚")

    except Exception as e:
        print(f"âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    scrape_numbers()
    import requests
from bs4 import BeautifulSoup
import pandas as pd

def get_numbers3():
    url = "https://www.hpfree.com/numbers/"
    response = requests.get(url)
    soup = BeautifulSoup(response.content, "html.parser")
    
    # ãƒŠãƒ³ãƒãƒ¼ã‚º3ã®å½“é¸ç•ªå·ã®å ´æ‰€ã‚’æŒ‡å®š
    numbers_section = soup.find_all("div", class_="numbers")[1]  # 148è¡Œç›®ã«å½“ãŸã‚‹éƒ¨åˆ†

    # ç”»åƒã‹ã‚‰altå±æ€§ã‚’å–å¾—
    images = numbers_section.find_all("img")
    
    # å½“é¸ç•ªå·ï¼ˆæœ€åˆã®3ã¤ã®imgã‚¿ã‚°ã®altå±æ€§ã‚’å–å¾—ï¼‰
    numbers3 = [img["alt"] for img in images[0:3]]  # ç”»åƒã®æœ€åˆã®3ã¤ï¼ˆaltå±æ€§ï¼‰ã‚’å–å¾—
    
    # å›å·ã¨æŠ½é¸æ—¥ã‚’å–å¾—
    title_section = soup.find("h1", class_="topTitle")
    title_text = title_section.text.strip()
    round_info = title_text.split("ç¬¬")[1].split("å›")
    round_number = round_info[0]
    draw_date = round_info[1].strip()

    # è³é‡‘ãƒ‡ãƒ¼ã‚¿ã®å–å¾—
    prize_section = soup.find_all("section", id="table-Area")[1]
    table = prize_section.find("table", class_="table1")
    rows = table.find_all("tr")

    prize_details = []
    for row in rows[1:]:  # æœ€åˆã®è¡Œï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼è¡Œï¼‰ã¯ã‚¹ã‚­ãƒƒãƒ—
        cols = row.find_all("td")
        if len(cols) == 2:  # æ­£ã—ã„è¡Œã¯2ã¤ã®åˆ—ã‚’æŒã£ã¦ã„ã‚‹å ´åˆ
            prize_details.append({
                "ã‚¿ã‚¤ãƒ—": row.find("th").text.strip(),
                "å½“é¸å£æ•°": cols[0].text.strip(),
                "å½“é¸é‡‘é¡": cols[1].text.strip(),
            })

    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å¤‰æ›
    data = {
        "å½“é¸ç•ªå·": "-".join(numbers3),
        "å›å·": round_number,
        "æŠ½é¸æ—¥": draw_date,
    }

    # å£æ•°ã¨å½“é¸é‡‘é¡ã‚’ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ 
    for prize in prize_details:
        prize_type = prize["ã‚¿ã‚¤ãƒ—"]
        data[f"{prize_type} å£æ•°"] = prize["å½“é¸å£æ•°"]
        data[f"{prize_type} å½“é¸é‡‘é¡"] = prize["å½“é¸é‡‘é¡"]

    # pandas DataFrameã«å¤‰æ›
    df = pd.DataFrame([data])

    # CSVã«ä¿å­˜
    data_dir = "./data"  # ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¨­å®š
    os.makedirs(data_dir, exist_ok=True)
    csv_path = f"{data_dir}/numbers_3_latest.csv"
    df.to_csv(csv_path, index=False, encoding="utf-8-sig")

    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãŒ {csv_path} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚")

# å®Ÿè¡Œ
get_numbers3()